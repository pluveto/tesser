---
title: Data Engineering
description: Acquire, validate, and store high quality market data.
---

## Sources

- **Bybit/Binance Historical** – Use `tesser-cli data download` for candles or `tesser-cli data download-trades` for tick data.
  - Prefer partitioned ticks: `--partition-by-day --resume-output --resume-archives` writes one parquet per trading day, keeps memory bounded, and can resume safely.
  - Single-file ticks: `--no-partition` writes one parquet for the full range (can be extremely large and does not support resuming partial output).
  - `--archive-cache-dir` optionally controls where downloaded `.csv.gz`/`.zip` archives are cached on disk (defaults under `data_path/ticks/cache/...`).
- **Custom Feeds** – Drop CSV/JSONL files under `data/`, run `tesser-cli data normalize` to convert them into the canonical Parquet schema, then point backtests to those `.parquet` outputs via `--data`.

## Validation

```bash
cargo run -p tesser-cli -- data validate \
    --path data/btcusdt_1m_sample.csv
```

The validator detects:

- Missing candles or duplicated timestamps.
- Price spikes outside configurable tolerances.
- Volume gaps that usually signal exchange outages.

## Storage

- CSV is used for quick diagnostics and as the staging format before normalization.
- Canonical candle datasets live in Parquet files emitted by `tesser-cli data normalize`, matching the schema replayed by the backtester.
- Tick downloads are written as Parquet using the exact schema produced by the flight recorder, so the backtester can consume them without conversion.

Keep raw downloads immutable and derive normalized datasets via scripts in `research/` so simulations remain reproducible.
